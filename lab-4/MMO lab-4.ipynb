{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyA504hNgEIdPKZrnBvvBv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import gym\n","from IPython.display import clear_output\n","from time import sleep\n","\n","# Создайте окружение, указав использование нового API Step\n","env = gym.make('Taxi-v3', new_step_api=True)\n","env.reset (seed=42)\n","\n","# Получение состояния среды и пространства действий\n","num_states = env.observation_space.n\n","num_actions = env.action_space.n\n","def print_frames(frames):\n","  for i, frame in enumerate(frames):\n","    clear_output(wait=True)\n","    print(frame['frame'])\n","    print(f\"Timestep: {i + 1}\")\n","    print(f\"State: {frame['state']}\")\n","    print(f\"Action: {frame['action']}\")\n","    print(f\"Reward: {frame['reward']}\")\n","    sleep(.1)\n","\n","def policy_evaluation(policy, env, discount_factor=0.9, theta=1e-8):\n","  V = np. zeros(num_states)\n","  while True:\n","    delta = 0\n","    for s in range(num_states):\n","      v = 0\n","      for a, action_prob in enumerate(policy[s]):\n","        for prob, next_state, reward, done in env.P[s][a]:\n","          v += action_prob * prob * (reward + discount_factor * V[next_state])\n","      delta = max(delta, np.abs(v - V[s]))\n","      V[s]=v\n","    if delta < theta:\n","      break\n","  return V\n","\n","def policy_improvement(V, env, discount_factor=0.9):\n","  policy = np. zeros([num_states, num_actions])\n","  for s in range(num_states):\n","    q = np. zeros (num_actions)\n","    for a in range(num_actions):\n","      for prob, next_state, reward, done in env. P[s][a]:\n","        q[a] += prob * (reward + discount_factor * V[next_state])\n","    best_a = np. argmax(q)\n","    policy[s, best_a] = 1.0\n","  return policy\n","\n","\n","def policy_iteration(env, discount_factor=0.9, max_iterations=200) :\n","  policy = np.ones([num_states, num_actions]) / num_actions\n","  for i in range(max_iterations):\n","    V = policy_evaluation(policy, env, discount_factor)\n","    new_policy = policy_improvement(V, env, discount_factor)\n","    if (new_policy == policy).all():\n","      print(f\"Policy converged at iteration (i+1).\")\n","      break\n","    policy = new_policy\n","  return policy, V\n","\n","policy, V = policy_iteration(env)\n","\n","\n","state = env. reset()\n","done = False\n","frames = []\n","\n","while not done:\n","  action = np.argmax(policy[state])\n","  state, reward, done, truncated, info = env.step(action)\n","  frames.append({\n","    'frame': env.render (mode='ansi'),\n","    'state': state,\n","    'action': action,\n","    'reward': reward\n","  })\n","\n","print_frames(frames)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ff5kcllWn7yJ","executionInfo":{"status":"ok","timestamp":1717092489611,"user_tz":-180,"elapsed":56464,"user":{"displayName":"Чжэнянь Дун (aqwddda)","userId":"02421885808330549764"}},"outputId":"a8b8cec8-761a-4e0d-e196-4a26b6889a14"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 14\n","State: 410\n","Action: 5\n","Reward: 20\n"]}]}]}